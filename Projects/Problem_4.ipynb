{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"},"colab":{"name":"Problem_4.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"a6db6be9f2404ed08a92e4d501d63200":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_529e2d58de2e4a3ab342b212a1c9ca6d","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_a9d3886589204b1dbf238bd0d611a745","IPY_MODEL_5a735a06a2b04cc4a1c88cc935f4c250"]}},"529e2d58de2e4a3ab342b212a1c9ca6d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"a9d3886589204b1dbf238bd0d611a745":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_89d599d8bcdf4c9680167912a56def3d","_dom_classes":[],"description":"100%","_model_name":"FloatProgressModel","bar_style":"success","max":102502400,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":102502400,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_a488edface3042b2930d01f702b6d109"}},"5a735a06a2b04cc4a1c88cc935f4c250":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_532c8e4e31b34cd6a9a0a815e7251c0b","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 97.8M/97.8M [00:37&lt;00:00, 2.76MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_6a35cbb820474c9faa18de58952da204"}},"89d599d8bcdf4c9680167912a56def3d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"a488edface3042b2930d01f702b6d109":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"532c8e4e31b34cd6a9a0a815e7251c0b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"6a35cbb820474c9faa18de58952da204":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"YpEmYZZ4jFK6"},"source":["**Name:** Luan Minh Tran\n","\n","**NetID:** lmt185\n","\n","**RUID:** 197002611"]},{"cell_type":"markdown","metadata":{"id":"f8f4Ln9HjHEK"},"source":["### Problem Statement\n","Using PyTorch, set up the pre-trained\n","network ResNet50. Obtain 10 of your own images that are similar to ImageNet classes\n","and classify them. Choose 10 images from 5 different classes (2 images per class).\n","Report the confusion matrix, the accuracy, the f-score, precision and recall of your\n","classifier. There should be 6 classes representing the 5 classes that your images belong\n","to as well as an 6th ’other’ class."]},{"cell_type":"markdown","metadata":{"id":"_LOCVPeajHEL"},"source":["### Load required modules"]},{"cell_type":"code","metadata":{"id":"CfAWjVT-jHEL","executionInfo":{"status":"ok","timestamp":1603331328843,"user_tz":240,"elapsed":4233,"user":{"displayName":"Luan Tran","photoUrl":"","userId":"09247676902623340038"}}},"source":["import torch\n","import os\n","from skimage import io, transform\n","from torch.utils.data import Dataset, DataLoader\n","import torchvision.models as models\n","import torchvision.datasets as datasets\n","import torchvision.transforms as transforms\n","import torch.nn.functional as F\n","import matplotlib.pyplot as plt\n","from PIL import Image\n","import numpy as np\n","import itertools"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"BT4yEsILjHEP","executionInfo":{"status":"ok","timestamp":1603331341336,"user_tz":240,"elapsed":16709,"user":{"displayName":"Luan Tran","photoUrl":"","userId":"09247676902623340038"}},"outputId":"4f411861-a43f-455a-8a30-5f7dca2e34e8","colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["a6db6be9f2404ed08a92e4d501d63200","529e2d58de2e4a3ab342b212a1c9ca6d","a9d3886589204b1dbf238bd0d611a745","5a735a06a2b04cc4a1c88cc935f4c250","89d599d8bcdf4c9680167912a56def3d","a488edface3042b2930d01f702b6d109","532c8e4e31b34cd6a9a0a815e7251c0b","6a35cbb820474c9faa18de58952da204"]}},"source":["# Load pretrained ResNet50 model\n","# https://pytorch.org/docs/stable/torchvision/models.html\n","# Tip: When loading model make sure pretrain argument set to True\n","# Tip: Good resource for PyTorch projects: https://github.com/pytorch/examples/blob/master/mnist/main.py\n","\n","device = torch.device('cuda')  # use gpu device\n","model = models.resnet50(pretrained=True) # load model from torchvision.models\n","model = model.to(device)  # model operations are sent to GPU\n","model.eval()"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Downloading: \"https://download.pytorch.org/models/resnet50-19c8e357.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-19c8e357.pth\n"],"name":"stderr"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a6db6be9f2404ed08a92e4d501d63200","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=102502400.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["ResNet(\n","  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n","  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (relu): ReLU(inplace=True)\n","  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n","  (layer1): Sequential(\n","    (0): Bottleneck(\n","      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (downsample): Sequential(\n","        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): Bottleneck(\n","      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","    )\n","    (2): Bottleneck(\n","      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","    )\n","  )\n","  (layer2): Sequential(\n","    (0): Bottleneck(\n","      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (downsample): Sequential(\n","        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): Bottleneck(\n","      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","    )\n","    (2): Bottleneck(\n","      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","    )\n","    (3): Bottleneck(\n","      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","    )\n","  )\n","  (layer3): Sequential(\n","    (0): Bottleneck(\n","      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (downsample): Sequential(\n","        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): Bottleneck(\n","      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","    )\n","    (2): Bottleneck(\n","      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","    )\n","    (3): Bottleneck(\n","      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","    )\n","    (4): Bottleneck(\n","      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","    )\n","    (5): Bottleneck(\n","      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","    )\n","  )\n","  (layer4): Sequential(\n","    (0): Bottleneck(\n","      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (downsample): Sequential(\n","        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): Bottleneck(\n","      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","    )\n","    (2): Bottleneck(\n","      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","    )\n","  )\n","  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n","  (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",")"]},"metadata":{"tags":[]},"execution_count":2}]},{"cell_type":"code","metadata":{"id":"KoYon9j6wXAg","executionInfo":{"status":"ok","timestamp":1603331362000,"user_tz":240,"elapsed":37365,"user":{"displayName":"Luan Tran","photoUrl":"","userId":"09247676902623340038"}},"outputId":"b5ec0896-67e8-4fae-b844-887b59eb3003","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["from google.colab import drive\n","\n","drive.mount('/content/gdrive')"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"de-J_Jj-E5Jx","executionInfo":{"status":"ok","timestamp":1603331362004,"user_tz":240,"elapsed":37366,"user":{"displayName":"Luan Tran","photoUrl":"","userId":"09247676902623340038"}}},"source":["def loadImage(root_dir):\n","    dataset = []\n","    for label in os.listdir(root_dir+\"/images\"):\n","      \n","      dataset+=([[os.path.join(root_dir, 'images/'+label, file),label]\n","                      for file in os.listdir(root_dir + \"/images/\"+label)\n","                          if file.endswith('.jpg')])\n","    return dataset"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ynbsazj0jHES","executionInfo":{"status":"ok","timestamp":1603331362005,"user_tz":240,"elapsed":37360,"user":{"displayName":"Luan Tran","photoUrl":"","userId":"09247676902623340038"}}},"source":["# Create custom Dataset for your images\n","# https://pytorch.org/tutorials/beginner/data_loading_tutorial.html\n","\n","class CustomDataset(Dataset):\n","    def __init__(self, root_dir, transform=None):\n","        self.transform = transform\n","        self.root_dir = root_dir\n","        self.dataset = loadImage(root_dir)\n","        \n","        # TODO: load image paths and labels located in the root_dir\n","        # Tip: append image_path and label pairs into self.dataset\n","    \n","    def __len__(self):\n","        return len(self.dataset)\n","    \n","    def __getitem__(self, index):\n","        image_path, label = self.dataset[index]\n","        \n","        image = Image.open(image_path)\n","        \n","        if self.transform:\n","            image = self.transform(image)\n","        \n","        return image, label"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"Hn99tXjBM3Mu","executionInfo":{"status":"ok","timestamp":1603331363116,"user_tz":240,"elapsed":38467,"user":{"displayName":"Luan Tran","photoUrl":"","userId":"09247676902623340038"}}},"source":["# Load all path data,label\n","root_dir = '/content/gdrive/My Drive/Computer Vision/Projects/'\n","\n","transform= transforms.Compose(\n","    [transforms.Resize(size=(224, 224)),\n","     transforms.ToTensor(),\n","     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n","\n","\n","#Load custome dataset \n","custom_dataset = CustomDataset(root_dir, transform)\n","loader = DataLoader(custom_dataset, batch_size=1)"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"NJ_bPx8eel1w","executionInfo":{"status":"ok","timestamp":1603331366726,"user_tz":240,"elapsed":42067,"user":{"displayName":"Luan Tran","photoUrl":"","userId":"09247676902623340038"}},"outputId":"80dfcd73-4fef-48ac-ec4f-e3a5b65293d9","colab":{"base_uri":"https://localhost:8080/","height":185}},"source":["temp = []\n","for (image, label) in loader:\n","  temp.append(int(label[0]))\n","\n","label_t = torch.LongTensor(temp)\n","label_t = torch.unsqueeze(label_t, dim = 1)\n","print(label_t)"],"execution_count":7,"outputs":[{"output_type":"stream","text":["tensor([[ 71],\n","        [ 71],\n","        [ 76],\n","        [ 76],\n","        [107],\n","        [107],\n","        [340],\n","        [340],\n","        [576],\n","        [576]])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"oLOlK_tVjHEX","executionInfo":{"status":"ok","timestamp":1603331366727,"user_tz":240,"elapsed":42063,"user":{"displayName":"Luan Tran","photoUrl":"","userId":"09247676902623340038"}}},"source":["# # Cycle through custom dataset and pass data into the model\n","# for (image, target) in loader:\n","#     image = image.to(device)  # enable GPU operations for image\n","#     prediction = model(image)\n","#     scores = model(image) #probability for eac class\n","#     _, preds = torch.max(scores, dim=1)  # please see docs for an explanation\n","#     print(preds)\n","#     # TODO: compute metrics on prediction such as accuracy, precision, etc."],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"gi-2NL74kxrt","executionInfo":{"status":"ok","timestamp":1603331366728,"user_tz":240,"elapsed":42061,"user":{"displayName":"Luan Tran","photoUrl":"","userId":"09247676902623340038"}}},"source":["epsilons = [0, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3]\n","\n","# perturbed_image = image + epsilon∗sign(data_grad) =x + ϵ ∗ sign(∇xJ(θ,x,y))\n","\n","def fgsm_attack(image, epsilon, data_grad):\n","    # Collect the element-wise sign of the data gradient\n","    sign_data_grad = data_grad.sign()\n","    # Create the perturbed image by adjusting each pixel of the input image\n","    perturbed_image = image + epsilon*sign_data_grad\n","    # Adding clipping to maintain [0,1] range\n","    perturbed_image = torch.clamp(perturbed_image, 0, 1)\n","    # Return the perturbed image\n","    return perturbed_image"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"ctfytpWljHEZ","executionInfo":{"status":"ok","timestamp":1603331383046,"user_tz":240,"elapsed":290,"user":{"displayName":"Luan Tran","photoUrl":"","userId":"09247676902623340038"}}},"source":["def check_accuracy(loader, model, epsilon):\n","    num_correct = 0\n","    num_samples = 0\n","    adv_examples = []\n","    \n","    # with torch.no_grad():\n","    for (data, _), label in zip(loader, label_t):\n","        data = data.to(device=device)\n","        label = label.to(device=device)\n","        \n","        # Set requires_grad attribute of tensor. Important for Attack\n","        data.requires_grad = True\n","\n","        scores = model(data)\n","        init_pred = scores.max(1, keepdim=True)[1]\n","        # print(\"Hello init: \", init_pred)\n","        if init_pred.item() != label.item(): # if the intial prediction is wrong => no need to attack\n","          continue\n","\n","        loss = F.cross_entropy(scores, label)\n","\n","        # Zero all existing gradients\n","        model.zero_grad()\n","\n","        loss.backward() # Calculate gradients of model in backward pass\n","\n","        data_grad = data.grad.data # Collect datagrad\n","\n","        perturbed_data = fgsm_attack(data, epsilon, data_grad) # Perform attack\n","\n","        scores = model(perturbed_data) # Re-model with perturbed_data \n","\n","        final_pred = scores.max(1, keepdim=True)[1]\n","\n","        if final_pred.item() == label.item():\n","          num_correct += 1\n","\n","          # In case epsilon == 0\n","          if (len(adv_examples) < 10):\n","            adv_ex = perturbed_data.squeeze().detach().cpu().numpy()\n","            adv_examples.append( (init_pred.item(), final_pred.item(), adv_ex) )\n","        else:\n","          # Save for visualization later:\n","          if len(adv_examples) < 10:\n","            adv_ex = perturbed_data.squeeze().detach().cpu().numpy()\n","            adv_examples.append( (init_pred.item(), final_pred.item(), adv_ex) )\n","        # print(\"Hello final: \", final_pred)\n","        # print(label)\n","        # num_correct += (predictions == label).sum()\n","        # num_samples += predictions.size(0)\n","    final_acc = num_correct/float(len(loader))\n","    print(\"Epsilon: {}\\tTest Accuracy = {} / {} = {}\".format(epsilon, num_correct, len(loader), final_acc))\n","\n","    return final_acc, adv_examples"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"GVMwF5WM1HfZ","executionInfo":{"status":"ok","timestamp":1603331389256,"user_tz":240,"elapsed":4780,"user":{"displayName":"Luan Tran","photoUrl":"","userId":"09247676902623340038"}},"outputId":"5698f87d-20a0-4365-c582-63b3bf864b41","colab":{"base_uri":"https://localhost:8080/","height":134}},"source":["accuracies = []\n","examples = []\n","\n","# Run test for each epsilon\n","for eps in epsilons:\n","    acc, ex = check_accuracy(loader, model, eps)\n","    accuracies.append(acc)\n","    examples.append(ex)"],"execution_count":13,"outputs":[{"output_type":"stream","text":["Epsilon: 0\tTest Accuracy = 6 / 10 = 0.6\n","Epsilon: 0.05\tTest Accuracy = 4 / 10 = 0.4\n","Epsilon: 0.1\tTest Accuracy = 4 / 10 = 0.4\n","Epsilon: 0.15\tTest Accuracy = 4 / 10 = 0.4\n","Epsilon: 0.2\tTest Accuracy = 3 / 10 = 0.3\n","Epsilon: 0.25\tTest Accuracy = 3 / 10 = 0.3\n","Epsilon: 0.3\tTest Accuracy = 2 / 10 = 0.2\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"3PZ8Wro51rQ1","executionInfo":{"status":"ok","timestamp":1603331399212,"user_tz":240,"elapsed":6571,"user":{"displayName":"Luan Tran","photoUrl":"","userId":"09247676902623340038"}},"outputId":"0c33e3bc-f98a-479b-d4db-f927e496dd81","colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1Z-tXksiS3BSeGQFLpT-CP0j5tgB-VwFz"}},"source":["cnt = 0\n","plt.figure(figsize=(20,20))\n","for i in range(len(epsilons)):\n","    for j in range(len(examples[i])):\n","        cnt += 1\n","        plt.subplot(len(epsilons),len(examples[0]),cnt)\n","        plt.xticks([], [])\n","        plt.yticks([], [])\n","        if j == 0:\n","            plt.ylabel(\"Eps: {}\".format(epsilons[i]), fontsize=14)\n","        orig,adv,ex = examples[i][j]\n","        plt.title(\"{} -> {}\".format(orig, adv), fontsize = 14)\n","        plt.imshow(np.transpose(ex, (1,2,0)))\n","plt.tight_layout()\n","plt.show()"],"execution_count":14,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]}]}